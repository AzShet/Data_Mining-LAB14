{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c062a855",
   "metadata": {},
   "source": [
    "# Session 14: Assembly techniques\n",
    "Made by:\n",
    "\n",
    "**- Ruelas Flores, César Diego**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e06fcb",
   "metadata": {},
   "source": [
    "- **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d382e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración General ---\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Configuración del Logging ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    stream=sys.stdout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a91c0",
   "metadata": {},
   "source": [
    "## 1. VARIABLES GLOBALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a326b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET_COLUMN: La variable que queremos predecir.\n",
    "TARGET_COLUMN = 'actual_productivity'\n",
    "\n",
    "# TEST_SIZE: Proporción del dataset que se usará para el conjunto de prueba.\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# RANDOM_STATE: Semilla para la aleatoriedad, asegura reproducibilidad.\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d659e2",
   "metadata": {},
   "source": [
    "## 2. FUNCIONES (../src/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c008e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/utils.py\n",
    "import logging\n",
    "import pyarrow\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, BaggingRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def load_and_prepare_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga el dataset de UCI, lo combina y lo retorna como un DataFrame de pandas.\n",
    "    Usa Polars internamente para una operación de limpieza rápida.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame de pandas listo para el preprocesamiento.\n",
    "    \"\"\"\n",
    "    logging.info(\"Iniciando la carga de datos desde el repositorio UCI...\")\n",
    "    try:\n",
    "        garment_prod = fetch_ucirepo(id=597)\n",
    "        X_pd = garment_prod.data.features\n",
    "        y_pd = garment_prod.data.targets\n",
    "        \n",
    "        df_pd = pd.concat([X_pd, y_pd], axis=1)\n",
    "        \n",
    "        # --- Uso de Polars para una operación eficiente ---\n",
    "        # Convertimos a Polars para eliminar la columna de forma rápida y segura\n",
    "        df_pl = pl.from_pandas(df_pd)\n",
    "        if 'date' in df_pl.columns:\n",
    "            df_pl = df_pl.drop('date')\n",
    "            logging.info(\"Columna 'date' eliminada usando Polars.\")\n",
    "            \n",
    "        # Regresamos a pandas para continuar el flujo de trabajo\n",
    "        df_final_pd = df_pl.to_pandas()\n",
    "        # -------------------------------------------------\n",
    "        \n",
    "        logging.info(f\"Datos cargados exitosamente. Dimensiones del DataFrame: {df_final_pd.shape}\")\n",
    "        return df_final_pd\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al cargar o preparar los datos: {e}\")\n",
    "        raise\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputa los valores faltantes en un DataFrame de pandas usando la mediana.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame con posibles valores nulos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame sin valores nulos.\n",
    "    \"\"\"\n",
    "    logging.info(\"Iniciando tratamiento de datos faltantes.\")\n",
    "    missing_counts = df.isnull().sum().sum()\n",
    "    \n",
    "    if missing_counts == 0:\n",
    "        logging.info(\"No se encontraron valores faltantes.\")\n",
    "        return df\n",
    "\n",
    "    logging.info(f\"Total de valores faltantes encontrados: {missing_counts}\")\n",
    "    \n",
    "    # Iterar sobre columnas con nulos y rellenar con la mediana\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "            logging.info(f\"Valores nulos en '{col}' imputados con la mediana ({median_val}).\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def treat_outliers_iqr(df: pd.DataFrame, num_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Trata los outliers en columnas numéricas usando el método de capping por IQR.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame de entrada.\n",
    "        num_cols (List[str]): Lista de columnas numéricas a tratar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con outliers tratados.\n",
    "    \"\"\"\n",
    "    logging.info(\"Iniciando tratamiento de outliers a nivel univariado (IQR).\")\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    for col in num_cols:\n",
    "        Q1 = df_out[col].quantile(0.25)\n",
    "        Q3 = df_out[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers_count = ((df_out[col] < lower_bound) | (df_out[col] > upper_bound)).sum()\n",
    "        \n",
    "        if outliers_count > 0:\n",
    "            logging.info(f\"Tratando {outliers_count} outliers en la columna '{col}'.\")\n",
    "            df_out[col] = np.clip(df_out[col], lower_bound, upper_bound)\n",
    "            \n",
    "    logging.info(\"Tratamiento de outliers finalizado.\")\n",
    "    return df_out\n",
    "\n",
    "def convert_to_dummies(df: pd.DataFrame, cat_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte las columnas categóricas a variables dummy usando pandas.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame de entrada.\n",
    "        cat_cols (List[str]): Lista de columnas categóricas.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con variables dummy.\n",
    "    \"\"\"\n",
    "    logging.info(\"Convirtiendo variables categóricas a dummies...\")\n",
    "    if not cat_cols:\n",
    "        logging.info(\"No hay columnas categóricas para convertir.\")\n",
    "        return df\n",
    "    \n",
    "    df_dummies = pd.get_dummies(df, columns=cat_cols, drop_first=True, dtype=float)\n",
    "    logging.info(f\"Columnas convertidas a dummies: {cat_cols}\")\n",
    "    return df_dummies\n",
    "\n",
    "def train_and_evaluate_models(\n",
    "    X_train: pd.DataFrame, \n",
    "    y_train: pd.Series, \n",
    "    X_test: pd.DataFrame, \n",
    "    y_test: pd.Series, \n",
    "    random_state: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Entrena y evalúa varios modelos de ensamblaje.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Características de entrenamiento.\n",
    "        y_train (pd.Series): Objetivo de entrenamiento.\n",
    "        X_test (pd.DataFrame): Características de prueba.\n",
    "        y_test (pd.Series): Objetivo de prueba.\n",
    "        random_state (int): Semilla aleatoria para reproducibilidad.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame con las métricas de evaluación para cada modelo.\n",
    "    \"\"\"\n",
    "    logging.info(\"Iniciando entrenamiento y evaluación de modelos de ensamblaje.\")\n",
    "    \n",
    "    # --- Definición de modelos base con hiperparámetros ---\n",
    "    r1 = LinearRegression()\n",
    "    r2 = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=random_state)\n",
    "    r3 = KNeighborsRegressor(n_neighbors=10)\n",
    "    r4 = SVR(kernel='rbf', C=1.0)\n",
    "    r5 = DecisionTreeRegressor(max_depth=5, random_state=random_state)\n",
    "\n",
    "    # --- Modelos de Ensamblaje ---\n",
    "    models = {\n",
    "        'Voting': VotingRegressor(estimators=[('lr', r1), ('rf', r2), ('knn', r3)]),\n",
    "        'Bagging': BaggingRegressor(estimator=r5, n_estimators=50, random_state=random_state),\n",
    "        'Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=random_state),\n",
    "        'Stacking': StackingRegressor(estimators=[('rf', r2), ('knn', r3), ('svr', r4)], final_estimator=r1)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        logging.info(f\"Entrenando el modelo: {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        logging.info(f\"Evaluando el modelo: {name}...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        results.append({\n",
    "            \"Modelo\": name,\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": np.sqrt(mse),\n",
    "            \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "            \"R2 Score\": r2_score(y_test, y_pred)\n",
    "        })\n",
    "    logging.info(\"Evaluación de todos los modelos completada.\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457f9a7",
   "metadata": {},
   "source": [
    "## 3. IMPLEMENTACIÓN PRINCIPAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79ac4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:20:23,980 - INFO - --- INICIANDO LA IMPLEMENTACIÓN PRINCIPAL ---\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"--- INICIANDO LA IMPLEMENTACIÓN PRINCIPAL ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1542b",
   "metadata": {},
   "source": [
    "--- Configuración de la ruta del proyecto ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5593c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:20:25,644 - INFO - Ruta del proyecto 'c:\\Users\\AzShet\\Documents\\Jupyter_LAB\\jupyter_projects\\5to_ciclo\\DataMining\\lab14\\Data_Mining-LAB14' agregada al sys.path.\n",
      "2025-06-19 15:20:26,180 - INFO - Módulo 'utils' importado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración de la ruta del proyecto ---\n",
    "# Esto es crucial para poder importar nuestro módulo 'utils'.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "        logging.info(f\"Ruta del proyecto '{project_root}' agregada al sys.path.\")\n",
    "    \n",
    "    # Esta importación ahora funcionará porque la celda anterior creó el archivo.\n",
    "    from src import utils\n",
    "    logging.info(\"Módulo 'utils' importado exitosamente.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error en la configuración inicial: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca83207",
   "metadata": {},
   "source": [
    "### 3.1 Parte A: Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904605c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:20:31,714 - INFO - --- [PARTE A] Iniciando preprocesamiento ---\n",
      "2025-06-19 15:20:31,716 - INFO - Iniciando la carga de datos desde el repositorio UCI...\n",
      "2025-06-19 15:20:33,383 - INFO - Columna 'date' eliminada usando Polars.\n",
      "2025-06-19 15:20:33,391 - INFO - Datos cargados exitosamente. Dimensiones del DataFrame: (1197, 14)\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"--- [PARTE A] Iniciando preprocesamiento ---\")\n",
    "# Cargar datos usando la función de utils\n",
    "df = utils.load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f1df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:20:55,428 - INFO - Columnas categóricas: ['quarter', 'department', 'day']\n",
      "2025-06-19 15:20:55,430 - INFO - Columnas numéricas: ['team', 'targeted_productivity', 'smv', 'wip', 'over_time', 'incentive', 'idle_time', 'idle_men', 'no_of_style_change', 'no_of_workers']\n"
     ]
    }
   ],
   "source": [
    "# Identificar columnas numéricas y categóricas (excluyendo el target)\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(TARGET_COLUMN)\n",
    "\n",
    "logging.info(f\"Columnas categóricas: {cat_cols}\")\n",
    "logging.info(f\"Columnas numéricas: {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faed8b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:20:58,894 - INFO - Iniciando tratamiento de datos faltantes.\n",
      "2025-06-19 15:20:58,897 - INFO - Total de valores faltantes encontrados: 506\n",
      "2025-06-19 15:20:58,901 - INFO - Valores nulos en 'wip' imputados con la mediana (1039.0).\n",
      "2025-06-19 15:20:58,904 - INFO - Convirtiendo variables categóricas a dummies...\n",
      "2025-06-19 15:20:58,915 - INFO - Columnas convertidas a dummies: ['quarter', 'department', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Pipeline de preprocesamiento usando las funciones de utils\n",
    "df_processed = (\n",
    "    df.pipe(utils.handle_missing_values)\n",
    "    .pipe(utils.convert_to_dummies, cat_cols=cat_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a75d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:21:09,519 - INFO - Iniciando tratamiento de outliers a nivel univariado (IQR).\n",
      "2025-06-19 15:21:09,527 - INFO - Tratando 79 outliers en la columna 'targeted_productivity'.\n",
      "2025-06-19 15:21:09,534 - INFO - Tratando 358 outliers en la columna 'wip'.\n",
      "2025-06-19 15:21:09,542 - INFO - Tratando 1 outliers en la columna 'over_time'.\n",
      "2025-06-19 15:21:09,547 - INFO - Tratando 11 outliers en la columna 'incentive'.\n",
      "2025-06-19 15:21:09,551 - INFO - Tratando 18 outliers en la columna 'idle_time'.\n",
      "2025-06-19 15:21:09,557 - INFO - Tratando 18 outliers en la columna 'idle_men'.\n",
      "2025-06-19 15:21:09,564 - INFO - Tratando 147 outliers en la columna 'no_of_style_change'.\n",
      "2025-06-19 15:21:09,578 - INFO - Tratando 210 outliers en la columna 'quarter_Quarter3'.\n",
      "2025-06-19 15:21:09,582 - INFO - Tratando 248 outliers en la columna 'quarter_Quarter4'.\n",
      "2025-06-19 15:21:09,587 - INFO - Tratando 44 outliers en la columna 'quarter_Quarter5'.\n",
      "2025-06-19 15:21:09,594 - INFO - Tratando 187 outliers en la columna 'day_Saturday'.\n",
      "2025-06-19 15:21:09,600 - INFO - Tratando 203 outliers en la columna 'day_Sunday'.\n",
      "2025-06-19 15:21:09,606 - INFO - Tratando 199 outliers en la columna 'day_Thursday'.\n",
      "2025-06-19 15:21:09,613 - INFO - Tratando 201 outliers en la columna 'day_Tuesday'.\n",
      "2025-06-19 15:21:09,619 - INFO - Tratando 208 outliers en la columna 'day_Wednesday'.\n",
      "2025-06-19 15:21:09,623 - INFO - Tratamiento de outliers finalizado.\n",
      "2025-06-19 15:21:09,624 - INFO - --- [PARTE A] Preprocesamiento finalizado ---\n"
     ]
    }
   ],
   "source": [
    "# Las nuevas columnas dummy son numéricas, actualizamos la lista para tratar outliers\n",
    "final_num_cols = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
    "final_num_cols.remove(TARGET_COLUMN)\n",
    "\n",
    "df_processed = utils.treat_outliers_iqr(df_processed, num_cols=final_num_cols)\n",
    "logging.info(\"--- [PARTE A] Preprocesamiento finalizado ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b02fe6",
   "metadata": {},
   "source": [
    "### 3.2. Parte B: Separación y Escalado de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d567831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:21:51,374 - INFO - --- [PARTE B] Iniciando separación y escalado ---\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"--- [PARTE B] Iniciando separación y escalado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a0eb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características (X) y objetivo (y)\n",
    "X = df_processed.drop(columns=TARGET_COLUMN)\n",
    "y = df_processed[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf56f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:21:56,498 - INFO - Datos divididos: 957 para entrenamiento, 240 para prueba.\n"
     ]
    }
   ],
   "source": [
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "logging.info(f\"Datos divididos: {len(X_train)} para entrenamiento, {len(X_test)} para prueba.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bc4cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar características numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9181874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:22:00,213 - INFO - --- [PARTE B] Escalado de características finalizado ---\n"
     ]
    }
   ],
   "source": [
    "# Convertir de vuelta a DataFrame para mantener la consistencia\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "logging.info(\"--- [PARTE B] Escalado de características finalizado ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6cc3b",
   "metadata": {},
   "source": [
    "### 3.3. Parte C: Entrenamiento y Evaluación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ec11a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:22:08,194 - INFO - --- [PARTE C] Iniciando entrenamiento y evaluación de modelos ---\n",
      "2025-06-19 15:22:08,195 - INFO - Iniciando entrenamiento y evaluación de modelos de ensamblaje.\n",
      "2025-06-19 15:22:08,196 - INFO - Entrenando el modelo: Voting...\n",
      "2025-06-19 15:22:08,384 - INFO - Evaluando el modelo: Voting...\n",
      "2025-06-19 15:22:13,015 - INFO - Entrenando el modelo: Bagging...\n",
      "2025-06-19 15:22:13,186 - INFO - Evaluando el modelo: Bagging...\n",
      "2025-06-19 15:22:13,199 - INFO - Entrenando el modelo: Boosting...\n",
      "2025-06-19 15:22:13,357 - INFO - Evaluando el modelo: Boosting...\n",
      "2025-06-19 15:22:13,362 - INFO - Entrenando el modelo: Stacking...\n",
      "2025-06-19 15:22:14,382 - INFO - Evaluando el modelo: Stacking...\n",
      "2025-06-19 15:22:14,418 - INFO - Evaluación de todos los modelos completada.\n",
      "2025-06-19 15:22:14,421 - INFO - --- [PARTE C] Evaluación de modelos finalizada ---\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"--- [PARTE C] Iniciando entrenamiento y evaluación de modelos ---\")\n",
    "model_results_df = utils.train_and_evaluate_models(\n",
    "    X_train, y_train, X_test, y_test, random_state=RANDOM_STATE\n",
    ")\n",
    "logging.info(\"--- [PARTE C] Evaluación de modelos finalizada ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbe537",
   "metadata": {},
   "source": [
    "### 3.4. **Visualización de Resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac24072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 15:24:33,805 - INFO - --- RESULTADOS FINALES DE LA EVALUACIÓN ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Modelo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "MSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "R2 Score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7b6359ae-44d6-4c10-ac84-f3090209f302",
       "rows": [
        [
         "0",
         "Bagging",
         "0.013347683539711018",
         "0.11553217534397514",
         "0.07797489063469576",
         "0.4973090907454015"
        ],
        [
         "1",
         "Stacking",
         "0.014207292886141854",
         "0.11919434922068183",
         "0.07929495358239019",
         "0.46493509845861536"
        ],
        [
         "2",
         "Boosting",
         "0.014493308818789876",
         "0.1203881589642016",
         "0.0785216836637749",
         "0.45416337100370674"
        ],
        [
         "3",
         "Voting",
         "0.01477932107711614",
         "0.12157023104821402",
         "0.07982487558999347",
         "0.44339178192847584"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.115532</td>\n",
       "      <td>0.077975</td>\n",
       "      <td>0.497309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stacking</td>\n",
       "      <td>0.014207</td>\n",
       "      <td>0.119194</td>\n",
       "      <td>0.079295</td>\n",
       "      <td>0.464935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boosting</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.120388</td>\n",
       "      <td>0.078522</td>\n",
       "      <td>0.454163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Voting</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>0.121570</td>\n",
       "      <td>0.079825</td>\n",
       "      <td>0.443392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Modelo       MSE      RMSE       MAE  R2 Score\n",
       "0   Bagging  0.013348  0.115532  0.077975  0.497309\n",
       "1  Stacking  0.014207  0.119194  0.079295  0.464935\n",
       "2  Boosting  0.014493  0.120388  0.078522  0.454163\n",
       "3    Voting  0.014779  0.121570  0.079825  0.443392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "logging.info(\"--- RESULTADOS FINALES DE LA EVALUACIÓN ---\")\n",
    "\n",
    "# Ordenar resultados por R2 Score para una mejor visualización\n",
    "model_results_df_sorted = model_results_df.sort_values(by=\"R2 Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Mostrar la tabla de resultados en el output del notebook\n",
    "display(model_results_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d890d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 16:21:37,770 - INFO - \n",
      "--- CONCLUSIÓN ---\n",
      "El mejor modelo es 'Bagging' con los siguientes resultados:\n",
      "- R2 Score: 0.4973\n",
      "- RMSE:     0.1155\n",
      "\n",
      "Justificación: El modelo 'Bagging' se elige como el mejor al presentar el coeficiente de determinación (R2 Score) más alto.\n",
      "Esto indica que es el modelo que mejor explica la variabilidad de la productividad real de los empleados entre todas las opciones evaluadas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Justificación del mejor modelo\n",
    "best_model = model_results_df_sorted.iloc[0]\n",
    "logging.info(f\"\"\"\n",
    "--- CONCLUSIÓN ---\n",
    "El mejor modelo es '{best_model['Modelo']}' con los siguientes resultados:\n",
    "- R2 Score: {best_model['R2 Score']:.4f}\n",
    "- RMSE:     {best_model['RMSE']:.4f}\n",
    "\n",
    "Justificación: El modelo '{best_model['Modelo']}' se elige como el mejor al presentar el coeficiente de determinación (R2 Score) más alto.\n",
    "Esto indica que es el modelo que mejor explica la variabilidad de la productividad real de los empleados entre todas las opciones evaluadas.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003aaec8",
   "metadata": {},
   "source": [
    "## 4. TESTING (../tests/test_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a67b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../tests/test_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tests/test_utils.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir el directorio raíz del proyecto al path para encontrar el módulo src\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src import utils\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_df() -> pd.DataFrame:\n",
    "    \"\"\"Fixture que crea un DataFrame de pandas de ejemplo para las pruebas.\"\"\"\n",
    "    data = {\n",
    "        'num_col1': [1, 2, 3, 4, 100],\n",
    "        'num_col2': [10.0, 20.0, np.nan, 40.0, 50.0],\n",
    "        'cat_col': ['A', 'B', 'A', 'C', 'B'],\n",
    "        'target': [1.1, 2.2, 3.3, 4.4, 5.5]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def test_load_and_prepare_data():\n",
    "    \"\"\"Prueba que la carga de datos funciona y elimina la columna 'date'.\"\"\"\n",
    "    df = utils.load_and_prepare_data()\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    assert 'date' not in df.columns\n",
    "    assert 'actual_productivity' in df.columns\n",
    "\n",
    "def test_handle_missing_values(sample_df):\n",
    "    \"\"\"Prueba que los valores nulos son imputados correctamente.\"\"\"\n",
    "    df_imputed = utils.handle_missing_values(sample_df)\n",
    "    assert df_imputed['num_col2'].isnull().sum() == 0\n",
    "    # La mediana de [10, 20, 40, 50] es 30.0\n",
    "    assert df_imputed.loc[2, 'num_col2'] == 30.0\n",
    "\n",
    "def test_convert_to_dummies(sample_df):\n",
    "    \"\"\"Prueba la conversión a variables dummy.\"\"\"\n",
    "    df_dummies = utils.convert_to_dummies(sample_df, cat_cols=['cat_col'])\n",
    "    assert 'cat_col' not in df_dummies.columns\n",
    "    assert 'cat_col_B' in df_dummies.columns\n",
    "    assert 'cat_col_C' in df_dummies.columns\n",
    "    assert 'cat_col_A' not in df_dummies.columns\n",
    "\n",
    "def test_treat_outliers_iqr(sample_df):\n",
    "    \"\"\"Prueba que los outliers son tratados (capped).\"\"\"\n",
    "    num_cols = ['num_col1']\n",
    "    df_treated = utils.treat_outliers_iqr(sample_df.copy(), num_cols)\n",
    "    Q1 = sample_df['num_col1'].quantile(0.25) # 2.5\n",
    "    Q3 = sample_df['num_col1'].quantile(0.75) # 4.5 -> This is incorrect for [1,2,3,4,100] pandas quantile is different. Let's calculate manually.\n",
    "    # Sorted: [1, 2, 3, 4, 100]. Q1=2.0, Q3=4.0, IQR=2.0. Upper bound = 4.0 + 1.5*2.0 = 7.0\n",
    "    upper_bound = 4.0 + 1.5 * (4.0 - 2.0)\n",
    "    assert df_treated.loc[4, 'num_col1'] == upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2c33a",
   "metadata": {},
   "source": [
    "## 5. EJECUCIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4280266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- C:\\Users\\AzShet\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n",
      "rootdir: c:\\Users\\AzShet\\Documents\\Jupyter_LAB\\jupyter_projects\\5to_ciclo\\DataMining\\lab14\\Data_Mining-LAB14\n",
      "plugins: anyio-4.9.0, mock-3.14.1\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "..\\tests\\test_utils.py::test_load_and_prepare_data \u001b[32mPASSED\u001b[0m\u001b[32m                [ 25%]\u001b[0m\n",
      "..\\tests\\test_utils.py::test_handle_missing_values \u001b[32mPASSED\u001b[0m\u001b[32m                [ 50%]\u001b[0m\n",
      "..\\tests\\test_utils.py::test_convert_to_dummies \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 75%]\u001b[0m\n",
      "..\\tests\\test_utils.py::test_treat_outliers_iqr \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 6.73s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest ../ -v -p no:cacheprovider"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
